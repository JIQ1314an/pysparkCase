{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 一、加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .appName(\"RetailStatistics\")\\\n",
    "    .getOrCreate()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "root_path = \"D:\\\\大三下\\\\Big_Data_Application_Spark\\\\datasets\\\\\"\n",
    "retail_path = os.path.join(root_path, \"data.json\")\n",
    "# dropna删除缺失值，thresh =1【至少有一个】\n",
    "# 过滤单笔订单过大的数据【这些数据是测试数据】\n",
    "df = spark.read.\\\n",
    "    json(retail_path).\\\n",
    "    dropna(thresh=1,subset=[\"storeProvince\"]).\\\n",
    "    filter(\"storeProvince != 'null'\") \\\n",
    "    .filter(\"receivable < 10000\") \\\n",
    "    .select(\"storeProvince\", \"storeID\", \"receivable\", \"dateTS\", \"payType\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+----------+-------------+-------+\n",
      "| storeProvince|storeID|receivable|       dateTS|payType|\n",
      "+--------------+-------+----------+-------------+-------+\n",
      "|        湖南省|   4064|      22.5|1563758583000| alipay|\n",
      "|        湖南省|    718|       7.0|1546737450000| alipay|\n",
      "|        湖南省|   1786|      10.0|1546478081000|   cash|\n",
      "|        广东省|   3702|      10.5|1559133703000| wechat|\n",
      "|广西壮族自治区|   1156|      10.0|1548594458000|   cash|\n",
      "+--------------+-------+----------+-------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 二、任务要求\n",
    "#### 2.1 各省销售指标每个省份的总销售额统计"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "# withColumn(\"money\", F.round(\"money\", 2)) 保留两位小数\n",
    "province_sale_df = df.groupBy(\"storeProvince\") \\\n",
    "        .sum(\"receivable\") \\\n",
    "        .withColumnRenamed(\"sum(receivable)\", \"money\") \\\n",
    "        .withColumn(\"money\", F.round(\"money\", 2)) \\\n",
    "        .orderBy(\"money\", ascending=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "| storeProvince|     money|\n",
      "+--------------+----------+\n",
      "|        广东省|1713207.92|\n",
      "|        湖南省|1701303.53|\n",
      "|广西壮族自治区|  37828.22|\n",
      "|        北京市|  10926.91|\n",
      "+--------------+----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "province_sale_df.show(4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# 数据存储MySQL\n",
    "# java.sql.SQLException: No suitable driver【缺少mysql的jar包，复制jdk1.8.0_261\\jre\\lib\\ext目录】\n",
    "# useUnicode=true 使用统一编码\n",
    "# option(k, v)\n",
    "# k = dbtable, v = 表名\n",
    "province_sale_df.write.mode(\"overwrite\") \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:mysql://localhost:3306/db2?useUnicode=true&characterEncoding=utf-8&useSSL=false&allowPublicKeyRetrieval=true\") \\\n",
    "    .option(\"dbtable\", \"province_sale\") \\\n",
    "    .option(\"user\", \"root\") \\\n",
    "    .option(\"password\", \"123456\") \\\n",
    "    .option(\"encode\", \"utf8\") \\\n",
    "    .save()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.2 TOP5 销售省份中有多少家店铺日销售额 1000+(有过达到的即可)\n",
    "* 每个省份得出一个满足条件的店铺统计（也可能存在该省份没有满足条件的店铺）"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# 2.2.1 先获取TOP5销售省份\n",
    "top5_province_sale_df = province_sale_df.limit(5).select(\"storeProvince\").withColumnRenamed(\"storeProvince\", \"top5_province\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# 2.2.2 与原始的DF进行内连接操作，查出所有的销售数据\n",
    "top5_province_join_df = df.join(top5_province_sale_df, on=df[\"storeProvince\"]==top5_province_sale_df[\"top5_province\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+----------+-------------+-------+-------------+\n",
      "|storeProvince|storeID|receivable|       dateTS|payType|top5_province|\n",
      "+-------------+-------+----------+-------------+-------+-------------+\n",
      "|       湖南省|   4064|      22.5|1563758583000| alipay|       湖南省|\n",
      "|       湖南省|    718|       7.0|1546737450000| alipay|       湖南省|\n",
      "|       湖南省|   1786|      10.0|1546478081000|   cash|       湖南省|\n",
      "+-------------+-------+----------+-------------+-------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top5_province_join_df.show(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|       day|count|\n",
      "+----------+-----+\n",
      "|2019-06-19|    1|\n",
      "|2019-08-04|    1|\n",
      "|2018-12-13|    1|\n",
      "|2019-06-07|    2|\n",
      "|2019-08-07|    2|\n",
      "|2019-01-13|    1|\n",
      "|2019-01-19|    1|\n",
      "|2019-04-12|    1|\n",
      "|2019-04-13|    1|\n",
      "|2018-12-08|    3|\n",
      "|2019-01-08|    2|\n",
      "|2018-12-16|    1|\n",
      "|2019-03-03|    1|\n",
      "|2019-06-17|    1|\n",
      "|2018-12-24|    1|\n",
      "|2019-01-18|    1|\n",
      "|2019-01-05|    1|\n",
      "|2019-06-02|    1|\n",
      "|2019-07-23|    1|\n",
      "|2019-02-27|    1|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 查看storeID==1546这个店铺每天的交易次数\n",
    "top5_province_join_df.filter(\"storeID==1546\").groupBy(F.from_unixtime(df[\"dateTS\"].substr(0,10), \"yyyy-MM-dd\").alias(\"day\")).count().show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----+\n",
      "|storeID|       day|money|\n",
      "+-------+----------+-----+\n",
      "|   1546|2019-06-07|713.0|\n",
      "|   1546|2018-12-13| 50.0|\n",
      "|   1546|2019-01-13|145.0|\n",
      "|   1546|2019-08-07| 24.0|\n",
      "+-------+----------+-----+\n",
      "only showing top 4 rows\n",
      "\n",
      "+-------+----------+-----+\n",
      "|storeID|       day|money|\n",
      "+-------+----------+-----+\n",
      "|   1546|2019-06-07|713.0|\n",
      "+-------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top5_province_join_df.groupBy(\"storeID\", F.from_unixtime(df[\"dateTS\"].substr(0,10), \"yyyy-MM-dd\").alias(\"day\"))\\\n",
    "    .sum(\"receivable\").withColumnRenamed(\"sum(receivable)\", \"money\")\\\n",
    "    .filter(\"storeID==1546\").show(4)\n",
    "\n",
    "\n",
    "\n",
    "top5_province_join_df.groupBy(\"storeID\", F.from_unixtime(df[\"dateTS\"].substr(0,10), \"yyyy-MM-dd\").alias(\"day\"))\\\n",
    "    .sum(\"receivable\").withColumnRenamed(\"sum(receivable)\", \"money\")\\\n",
    "    .filter(\"storeID==1546\").dropDuplicates(subset=[\"storeID\"]).show()\n",
    "    # 存在多个相同的storeID时，只会选择第一个"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|storeID|         avg_money|\n",
      "+-------+------------------+\n",
      "|   3243|            2105.0|\n",
      "|   1460|            1903.0|\n",
      "|   3737|1600.6666666666667|\n",
      "|   2653|            1178.5|\n",
      "|    915|            1850.0|\n",
      "|   2555|           3294.98|\n",
      "|   2177|            1242.0|\n",
      "|    407|          1298.125|\n",
      "|   1595|            1696.0|\n",
      "|    769|1499.3333333333333|\n",
      "|   3247|            1650.5|\n",
      "|   1475|            1027.0|\n",
      "|   1926|1763.7142857142858|\n",
      "|   3288|            3000.0|\n",
      "|   3454|            2258.0|\n",
      "|   2118| 1209.321739130435|\n",
      "|   3654|            4409.0|\n",
      "|   3604|            1288.0|\n",
      "|   1865|          2701.098|\n",
      "|   3975|            1050.0|\n",
      "+-------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2.2.3 根据店铺分组后再根据天进行分组统计销售额\n",
    "# from_unixtime 的精度是秒级\n",
    "top5_province_join_df.groupBy(\"storeID\", F.from_unixtime(df[\"dateTS\"].substr(0,10), \"yyyy-MM-dd\").alias(\"day\"))\\\n",
    "    .sum(\"receivable\").withColumnRenamed(\"sum(receivable)\", \"money\")\\\n",
    "    .groupBy(\"storeID\")\\\n",
    "    .avg(\"money\").withColumnRenamed(\"avg(money)\", \"avg_money\")\\\n",
    "    .filter(\"avg_money > 1000\")\\\n",
    "    .show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|storeProvince|count|\n",
      "+-------------+-----+\n",
      "|       湖南省|    9|\n",
      "|       广东省|   17|\n",
      "|       上海市|    1|\n",
      "+-------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top5_province_join_df.groupBy(\"storeProvince\", \"storeID\", F.from_unixtime(df[\"dateTS\"].substr(0,10), \"yyyy-MM-dd\").alias(\"day\"))\\\n",
    "    .sum(\"receivable\").withColumnRenamed(\"sum(receivable)\", \"money\")\\\n",
    "    .groupBy(\"storeID\", \"storeProvince\")\\\n",
    "    .avg(\"money\").withColumnRenamed(\"avg(money)\", \"avg_money\")\\\n",
    "    .filter(\"avg_money > 1000\")\\\n",
    "    .groupBy(\"storeProvince\")\\\n",
    "    .count()\\\n",
    "    .show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "| storeProvince|count|\n",
      "+--------------+-----+\n",
      "|广西壮族自治区|    3|\n",
      "|        湖南省|   97|\n",
      "|        广东省|  105|\n",
      "|        上海市|    2|\n",
      "+--------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top5_province_join_df.groupBy(\"storeProvince\", \"storeID\", F.from_unixtime(df[\"dateTS\"].substr(0,10), \"yyyy-MM-dd\").alias(\"day\"))\\\n",
    "    .sum(\"receivable\").withColumnRenamed(\"sum(receivable)\", \"money\")\\\n",
    "    .filter(\"money > 1000\")\\\n",
    "    .dropDuplicates(subset=[\"storeID\"])\\\n",
    "    .groupBy(\"storeProvince\")\\\n",
    "    .count()\\\n",
    "    .show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "* 另一种求解方法"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|  min(day)|\n",
      "+----------+\n",
      "|1970-01-02|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean, min, max, to_date\n",
    "day = to_date(F.from_unixtime(df[\"dateTS\"].substr(0,10), \"yyyy-MM-dd\")).alias(\"day\")\n",
    "top5_province_join_df1 = top5_province_join_df.select(\"storeProvince\", \"storeID\", \"receivable\", day, \"payType\")\n",
    "top5_province_join_df1.agg(min(\"day\")).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "store_date_df = top5_province_join_df1\\\n",
    "    .groupBy(\"storeID\")\\\n",
    "    .agg(\n",
    "    min(\"day\").name(\"start_date\"),\n",
    "    max(\"day\").name(\"end_date\")\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+\n",
      "|storeID|start_date|  end_date|\n",
      "+-------+----------+----------+\n",
      "|    121|2019-03-18|2019-04-12|\n",
      "|    145|2018-11-20|2018-12-07|\n",
      "|    154|2018-12-01|2019-08-01|\n",
      "|    200|2018-11-17|2269-12-30|\n",
      "|    209|2018-11-20|2018-12-28|\n",
      "|    214|2019-02-18|2019-02-18|\n",
      "|    219|2019-01-27|2019-05-28|\n",
      "|    234|2018-12-04|2018-12-20|\n",
      "|    248|2018-11-18|2019-06-08|\n",
      "|    277|2018-11-17|2019-08-24|\n",
      "|    283|2019-03-06|2019-07-01|\n",
      "|    284|2018-11-23|2019-07-28|\n",
      "|    285|2019-01-08|2019-02-12|\n",
      "|    287|2018-12-17|2019-07-21|\n",
      "|    292|2019-02-12|2019-07-11|\n",
      "|    299|2018-12-01|2019-08-15|\n",
      "|    300|2018-11-23|2019-07-28|\n",
      "|    301|2018-12-01|2032-02-16|\n",
      "|    304|2018-11-19|2019-08-13|\n",
      "|    313|2018-11-27|2019-08-16|\n",
      "+-------+----------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "store_date_df.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+--------------------+\n",
      "|storeID|start_date|  end_date|            day_nums|\n",
      "+-------+----------+----------+--------------------+\n",
      "|    121|2019-03-18|2019-04-12|   INTERVAL '25' DAY|\n",
      "|    145|2018-11-20|2018-12-07|   INTERVAL '17' DAY|\n",
      "|    154|2018-12-01|2019-08-01|  INTERVAL '243' DAY|\n",
      "|    200|2018-11-17|2269-12-30|INTERVAL '91719' DAY|\n",
      "|    209|2018-11-20|2018-12-28|   INTERVAL '38' DAY|\n",
      "|    214|2019-02-18|2019-02-18|    INTERVAL '0' DAY|\n",
      "|    219|2019-01-27|2019-05-28|  INTERVAL '121' DAY|\n",
      "|    234|2018-12-04|2018-12-20|   INTERVAL '16' DAY|\n",
      "|    248|2018-11-18|2019-06-08|  INTERVAL '202' DAY|\n",
      "|    277|2018-11-17|2019-08-24|  INTERVAL '280' DAY|\n",
      "|    283|2019-03-06|2019-07-01|  INTERVAL '117' DAY|\n",
      "|    284|2018-11-23|2019-07-28|  INTERVAL '247' DAY|\n",
      "|    285|2019-01-08|2019-02-12|   INTERVAL '35' DAY|\n",
      "|    287|2018-12-17|2019-07-21|  INTERVAL '216' DAY|\n",
      "|    292|2019-02-12|2019-07-11|  INTERVAL '149' DAY|\n",
      "|    299|2018-12-01|2019-08-15|  INTERVAL '257' DAY|\n",
      "|    300|2018-11-23|2019-07-28|  INTERVAL '247' DAY|\n",
      "|    301|2018-12-01|2032-02-16| INTERVAL '4825' DAY|\n",
      "|    304|2018-11-19|2019-08-13|  INTERVAL '267' DAY|\n",
      "|    313|2018-11-27|2019-08-16|  INTERVAL '262' DAY|\n",
      "+-------+----------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date, to_timestamp, date_format\n",
    "\n",
    "store_date_df.withColumn(\"day_nums\", col=(to_date(store_date_df[\"end_date\"])- to_date(store_date_df[\"start_date\"]))).show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.3 TOP5 省份中各个省份的平均单单价"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "| storeProvince|avg_price|\n",
      "+--------------+---------+\n",
      "|        上海市|   613.21|\n",
      "|广西壮族自治区|    40.03|\n",
      "|        湖南省|    36.86|\n",
      "|        广东省|    32.81|\n",
      "|        北京市|    25.95|\n",
      "+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top5_province_join_df.groupBy(\"storeProvince\")\\\n",
    "        .avg(\"receivable\")\\\n",
    "        .withColumnRenamed(\"avg(receivable)\", \"avg_price\") \\\n",
    "        .withColumn(\"avg_price\", F.round(\"avg_price\", 2)) \\\n",
    "        .orderBy(\"avg_price\", ascending=False)\\\n",
    "        .show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.4 TOP5 省份中各个省份的支付类型比例"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "# 2.4.1 创建一个临时视图\n",
    "top5_province_join_df.createTempView(\"province_pay\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "def udf_percent(percent):\n",
    "    return str(round(percent*100, 2)) + \"%\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+-----+\n",
      "|storeProvince|payType|total|\n",
      "+-------------+-------+-----+\n",
      "|       上海市|   cash|   12|\n",
      "|       上海市| alipay|   12|\n",
      "|       上海市| wechat|   12|\n",
      "+-------------+-------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select storeProvince, payType, count(1) over(partition by storeProvince) as total from province_pay\n",
    "\"\"\").show(3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------+-------+\n",
      "| storeProvince| payType|percent|\n",
      "+--------------+--------+-------+\n",
      "|        上海市|    cash| 33.33%|\n",
      "|        上海市|  alipay|  8.33%|\n",
      "|        上海市|  wechat| 58.33%|\n",
      "|        北京市|    cash| 40.14%|\n",
      "|        北京市|  wechat| 46.08%|\n",
      "|        北京市|  alipay| 13.78%|\n",
      "|        广东省|  wechat| 39.48%|\n",
      "|        广东省|    cash| 52.91%|\n",
      "|        广东省|bankcard|  0.72%|\n",
      "|        广东省|  alipay|  6.88%|\n",
      "|广西壮族自治区|    cash| 73.23%|\n",
      "|广西壮族自治区|  wechat| 21.59%|\n",
      "|广西壮族自治区|  alipay|  4.23%|\n",
      "|广西壮族自治区|bankcard|  0.95%|\n",
      "|        湖南省|  alipay|  4.24%|\n",
      "|        湖南省|    cash| 70.81%|\n",
      "|        湖南省|  wechat| 24.88%|\n",
      "|        湖南省|bankcard|  0.06%|\n",
      "+--------------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType\n",
    "\n",
    "spark.udf.register(\"udf1\", udf_percent, StringType())\n",
    "spark.sql(\"\"\"\n",
    "select storeProvince, payType, udf1((count(payType)/ total)) as percent from\n",
    "(select storeProvince, payType, count(1) over(partition by storeProvince) as total from province_pay) subtable\n",
    "group by storeProvince, payType, total\n",
    "\"\"\").show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "ml",
   "language": "python",
   "display_name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}